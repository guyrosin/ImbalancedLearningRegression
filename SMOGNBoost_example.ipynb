{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IoG91SaLwtuq"
      },
      "source": [
        "# ImbalancedLearningRegression (0.0.1): Usage\n",
        "---\n",
        "## SMOGNBoost\n",
        "Amit Shanbhoug, 8677407 \\\n",
        "Adapted from Nick Kunz's SMOGN package: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_1_beg.ipynb\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSHCUHR--nw"
      },
      "source": [
        "## Installation\n",
        "\n",
        "First, we install ImbalancedLearningRegression from the Github repository. Alternatively, we could install from the official PyPI distribution. However, the developer version is utilized here for the latest release. Works on Kernel: python 3.10.7, there may be some issues if you run with a different version, older or newer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u9SqBgJ8rduy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## suppress install output\n",
        "\n",
        "## install pypi release\n",
        "# !pip install ImbalancedLearningRegression\n",
        "\n",
        "## install developer version\n",
        "# !pip install git+https://github.com/paobranco/ImbalancedLearningRegression.git"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gKhs9eJd_Ab6"
      },
      "source": [
        "## Dependencies\n",
        "Next, we load the required dependencies. Here we import `ImbalancedLearningRegression` to later apply Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise. In addition, we use `pandas` for data handling, and `seaborn` to visualize our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fPB6tSLinAFS"
      },
      "outputs": [],
      "source": [
        "## load dependencies\n",
        "## load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import math\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from ImbalancedLearningRegression.smogn import smogn\n",
        "from ImbalancedLearningRegression.smogn_boost import smogn_boost\n",
        "#from ImbalancedLearningRegression import *\n",
        "import ImbalancedLearningRegression as iblr\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "do8cJ25c_HcF"
      },
      "source": [
        "# Data Description\n",
        "The dataset is sourced from Kaggle with a usability score of 10.0 (Source: https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice). This data contains information about  and goal here is to predict whether an individual would be interested in vehicle insurance, denoted by response column \"response\". The target variable is highly imbalanced - 0: 319594, 1: 62531\n",
        "\n",
        "## Loading the Data\n",
        "Below, we load our data (Imbalanced Insurance Data set), In this case, we name our training set `data` and test data as 'test-data'. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RaFdQ2od-qVO"
      },
      "outputs": [],
      "source": [
        "#ds = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/housing.csv\")\n",
        "#ata = \"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/College.csv\"\n",
        "#test_data = \"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/CollegeTest.csv\"\n",
        "#print(len(ds))\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/housing.csv\")\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/housingTest.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D71nm6Co_KK-"
      },
      "source": [
        "## Introduction to SMOGNBoost\n",
        "Here we cover the focus of this example. We call the `smogn_boost` function from this package (`ImbalancedLearningRegression.smogn_boost`) and satisfy the minimum required arguments: `data` and `y`.\n",
        "\n",
        "* data: this argument takes a training data set\n",
        "* test_data: this argument takes a test data set\n",
        "* y: this argument takes a string, which specifies a response variable by header name \n",
        "* TotalIterations: this argument takes a positive integer, which specifies the total number of iterations\n",
        "* pert: perturbation / noise percentage\n",
        "* replace: sampling replacement (bool)\n",
        "* k: num of neighs for over-sampling (pos int)\n",
        "* error_threshold: this argument takes a positive integer, which specifies an error threshold \n",
        "* rel_thres: user defined relevance threshold \n",
        "* samp_method: \"balance or extreme\" - sampling method is perc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "## conduct smogn\n",
        "#smogn = smogn(data, y= \"SalePrice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qRV9hjPjJVF",
        "outputId": "9c4b115e-1e92-4f75-ecc5-f906fdf0cd6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:201: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\over_sampling_smogn.py:128: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.iloc[:, j] = pd.Categorical(pd.factorize(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dist_matrix: 100%|##########| 213/213 [00:25<00:00,  8.30it/s]\n",
            "synth_matrix: 100%|##########| 213/213 [00:04<00:00, 45.84it/s]\n",
            "r_index: 100%|##########| 83/83 [00:00<00:00, 87.74it/s]\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:289: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H\n",
            "        Id  MSSubClass MSZoning  LotArea Street LotShape LandContour  \\\n",
            "0     1058          60       FV    10395   Pave      Reg         Lvl   \n",
            "1     1056          60       RL    10386   Pave      Reg         Lvl   \n",
            "2      222          59       FV     8412   Pave      Reg         Lvl   \n",
            "3      207          60       FV     8453   Pave      Reg         Lvl   \n",
            "4      803          60       RL    10007   Pave      Reg         Lvl   \n",
            "...    ...         ...      ...      ...    ...      ...         ...   \n",
            "1438  1439          20       RM     7407   Pave      Reg         Lvl   \n",
            "1439  1440          60       RL    11584   Pave      Reg         Lvl   \n",
            "1440  1441          70       RL    11526   Pave      IR1         Bnk   \n",
            "1441  1442         120       RM     4426   Pave      Reg         Lvl   \n",
            "1442  1443          60       FV    11003   Pave      Reg         Lvl   \n",
            "\n",
            "     Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
            "0       AllPub    Inside       Gtl  ...          49             0         0   \n",
            "1       AllPub    Inside       Gtl  ...          49             0         0   \n",
            "2       AllPub    Inside       Gtl  ...         105             0         0   \n",
            "3       AllPub    Inside       Gtl  ...         108             0         0   \n",
            "4       AllPub    Inside       Gtl  ...          52             0         0   \n",
            "...        ...       ...       ...  ...         ...           ...       ...   \n",
            "1438    AllPub    Inside       Gtl  ...         158           158         0   \n",
            "1439    AllPub    Inside       Gtl  ...          88           216         0   \n",
            "1440    AllPub    Inside       Mod  ...           0             0         0   \n",
            "1441    AllPub    Inside       Gtl  ...           0             0         0   \n",
            "1442    AllPub    Inside       Gtl  ...          52             0         0   \n",
            "\n",
            "     ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
            "0              0        0        0       4    2009        WD        Normal  \n",
            "1              0        0        0       4    2009        WD        Normal  \n",
            "2              0        0        0       7    2008        WD        Normal  \n",
            "3              0        0        0       6    2009        WD        Normal  \n",
            "4              0        0        0       4    2008        WD        Normal  \n",
            "...          ...      ...      ...     ...     ...       ...           ...  \n",
            "1438           0        0        0       4    2010        WD        Normal  \n",
            "1439           0        0        0      11    2007        WD        Normal  \n",
            "1440           0        0        0       9    2008        WD        Normal  \n",
            "1441           0        0        0       5    2008        WD        Normal  \n",
            "1442           0        0        0       4    2009        WD        Normal  \n",
            "\n",
            "[1230 rows x 61 columns]\n",
            "0       252250\n",
            "1       252749\n",
            "2       252692\n",
            "3       252733\n",
            "4       275534\n",
            "         ...  \n",
            "1438    149700\n",
            "1439    197000\n",
            "1440    191000\n",
            "1441    149300\n",
            "1442    310000\n",
            "Name: SalePrice, Length: 1230, dtype: int64\n",
            "K\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'FV'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## conduct smogn_boost\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m smogn_boost \u001b[39m=\u001b[39m smogn_boost(data, test_data, y\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSalePrice\u001b[39;49m\u001b[39m\"\u001b[39;49m, TotalIterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:140\u001b[0m, in \u001b[0;36msmogn_boost\u001b[1;34m(data, test_data, y, TotalIterations, error_threshold, pert, replace, k, samp_method, drop_na_col, drop_na_row, rel_thres)\u001b[0m\n\u001b[0;32m    137\u001b[0m dt_model \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mDecisionTreeRegressor()\n\u001b[0;32m    139\u001b[0m \u001b[39m# train decision tree classifier\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m dt_model \u001b[39m=\u001b[39m dt_model\u001b[39m.\u001b[39;49mfit(x_oversampled, y_oversampled)\n\u001b[0;32m    142\u001b[0m \u001b[39m# predict the features in user provided data\u001b[39;00m\n\u001b[0;32m    143\u001b[0m dt_data_predictions \u001b[39m=\u001b[39m dt_model\u001b[39m.\u001b[39mpredict(X_data)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1314\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1343\u001b[0m         X,\n\u001b[0;32m   1344\u001b[0m         y,\n\u001b[0;32m   1345\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1346\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    173\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'FV'"
          ]
        }
      ],
      "source": [
        "## conduct smogn_boost\n",
        "smogn_boost = smogn_boost(data, test_data, y=\"SalePrice\", TotalIterations=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We read the test & training data and split them into features (X) and target value (Y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the test data and split features (X) and target value (Y)\n",
        "df_testData = pd.read_csv(test_data, header = 0)\n",
        "X_test = df_testData.drop(y, axis = 1)\n",
        "Y_test = df_testData[y]\n",
        "    \n",
        "# read the training data and split features (X) and target value (Y)\n",
        "df_data = pd.read_csv(data, header = 0)\n",
        "X_data = df_data.drop(y, axis = 1)\n",
        "Y_data = df_data[y]\n",
        "\n",
        "# set for clarity, name of target variable not data\n",
        "y_train = y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we set an initial iteration as well as initialize empty arrays for the result, beta values, and decision tree predictions based on x_test. The array will store values over each iteration and be used to calculate the result after the final iteration (total iterations specified by the user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set an initial iteration\n",
        "iteration = 1\n",
        "    \n",
        "# set an array of results, beta values, and decision tree predictions based on x_test\n",
        "result = np.empty(TotalIterations, dtype=int)\n",
        "beta = np.empty(TotalIterations, dtype=int)\n",
        "dt_test_predictions = np.empty(X_test, dtype=int)\n",
        "    \n",
        "# Dt(i) set distribution as 1/m weights, which is length of training data -1, as one of them is the target variable y \n",
        "weights = 1/(len(data))\n",
        "dt_distribution = np.zeros(len(data))\n",
        "for i in range(len(data)):\n",
        "    dt_distribution[i] = weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call phi control and specificially the control points as it will be used when we apply SMOGN to oversample the imbalanced training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calling phi control\n",
        "pc = phi_ctrl_pts (y=y, method=\"auto\", xtrm_type = \"both\", coeff = 1.5, ctrl_pts=None)\n",
        "    \n",
        "# calling only the control points (third value) from the output\n",
        "rel_ctrl_pts_rg = pc[2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we begin the main function which runs for the total iterations specified by the user.\n",
        "\n",
        "We obtain an oversampled dataset using SMOGN and our training dataset, split the oversampled data into features and a target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop while iteration is less than user provided iterations\n",
        "while iteration <= TotalIterations:\n",
        "\n",
        "# use initial training data set provided by user to obtain oversampled dataset using SMOGN, calculating it for the bumps\n",
        "    dt_over_sampled = smogn(data=data, y_train = y_train, k = 5, pert = pert, replace=replace, rel_thres = rel_thres, rel_method = \"manual\", rel_ctrl_pts_rg = rel_ctrl_pts_rg)\n",
        "\n",
        "# splitting oversampled data for subsequent training data use below\n",
        "    df_oversampled = dt_over_sampled, header = 0\n",
        "    x_oversampled = df_oversampled.drop(y_train, axis = 1)\n",
        "    y_oversampled = df_oversampled[y_train]\n",
        "    \n",
        "# calls the decision tree and use it to achieve a new model, predict regression value for y (target response variable), and return the predicted values\n",
        "    dt_model = tree.DecisionTreeRegressor()\n",
        "        \n",
        "# train decision tree classifier\n",
        "    dt_model = dt_model.fit(x_oversampled, y_oversampled)\n",
        "        \n",
        "# predict the features in user provided data\n",
        "    dt_data_predictions = dt_model.predict(X_data)\n",
        "        \n",
        "# predict the features in user provided test data\n",
        "    dt_test_predictions.append(dt_model.predict(X_test))\n",
        "\n",
        "# initialize model error rate & epsilon t value\n",
        "    model_error = np.zeros(len(dt_data_predictions))\n",
        "    epsilon_t = 0\n",
        "\n",
        "# calculate the model error rate of the new model achieved earlier, as the delta between original dataset and predicted oversampled dataset\n",
        "# for each y in the dataset, calculate whether it is greater/lower than threshold and update accordingly\n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        model_error[i] = abs((Y_data[i] - dt_data_predictions[i])/Y_data[i])\n",
        "        \n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        if model_error[i] > error_threshold:\n",
        "            epsilon_t = epsilon_t + dt_distribution[i]\n",
        "                                      \n",
        "# beta is the update parameter of weights based on the model error rate calculated\n",
        "    beta.append(pow(epsilon_t, 2))\n",
        "\n",
        "# update the distribution weights\n",
        "    for i in dt_distribution:\n",
        "        if model_error[i] <= error_threshold:\n",
        "            dt_distribution[i] = dt_distribution[i] * beta\n",
        "        else:\n",
        "            dt_distribution[i] = dt_distribution[i]\n",
        "\n",
        "# normalize the distribution \n",
        "    dt_normalized = preprocessing.normalize(dt_distribution, max)\n",
        "\n",
        "# iteration count\n",
        "    iteration += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we calculate the result outside the while loop. We split calculations into numerator and denominator calculating a series. To simplify the calculation, we use log(b) instead of (log(1/b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate result\n",
        "numer = 0\n",
        "denom = 0\n",
        "    \n",
        "for b, i in zip(beta, dt_test_predictions):\n",
        "    numer += math.log(b) * i\n",
        "    denom += math.log(b)\n",
        "return numer/denom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYCW_1t-zra"
      },
      "source": [
        "## Results\n",
        "After conducting SMOGNBoost, we briefly examine the results. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spqtcHX1yTfM",
        "outputId": "bfae806f-6e9c-4db7-a09d-0a7786e5d202"
      },
      "outputs": [],
      "source": [
        "## dimensions - original data \n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5_E-5oQF18"
      },
      "source": [
        "## Conclusion\n",
        "TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQ5iGDJa2LM"
      },
      "source": [
        "## References\n",
        "\n",
        "Branco, P., Torgo, L., Ribeiro, R. (2017). SMOGN: A Pre-Processing Approach for Imbalanced Regression. Proceedings of Machine Learning Research, 74:36-50. http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
        "\n",
        "Torgo, L., Ribeiro, R. P., Pfahringer, B., & Branco, P. (2013, September). Smote for regression. In Portuguese conference on artificial intelligence (pp. 378-389). Springer, Berlin, Heidelberg. https://researchcommons.waikato.ac.nz/bitstream/handle/10289/8518/smoteR.pdf?sequence=23\n",
        "\n",
        "Kunz, N. (2019). SMOGN: Synthetic Minority Over-Sampling for Regression with Gaussian Noise (Version 0.1.0). Python Package Index.\n",
        "https://pypi.org/project/smogn. \n",
        "\n",
        "Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.\n",
        "http://www-bcf.usc.edu/~gareth/ISL/data.html.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMOTE_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('ImbalancedLearningRegression')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "1c961f0653c6e1bdf33e63c7e72c10f24f14dece539fb6df7c7f98656204e204"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
