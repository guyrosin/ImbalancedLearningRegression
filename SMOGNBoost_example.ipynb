{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IoG91SaLwtuq"
      },
      "source": [
        "# ImbalancedLearningRegression (0.0.1): Usage\n",
        "---\n",
        "## SMOGNBoost\n",
        "Amit Shanbhoug, 8677407 \\\n",
        "Adapted from Nick Kunz's SMOGN package: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_1_beg.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSHCUHR--nw"
      },
      "source": [
        "## Installation\n",
        "\n",
        "First, we install ImbalancedLearningRegression from the Github repository. Alternatively, we could install from the official PyPI distribution. However, the developer version is utilized here for the latest release."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9SqBgJ8rduy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## suppress install output\n",
        "\n",
        "## install pypi release\n",
        "# !pip install ImbalancedLearningRegression\n",
        "\n",
        "## install developer version\n",
        "# !pip install git+https://github.com/paobranco/ImbalancedLearningRegression.git"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gKhs9eJd_Ab6"
      },
      "source": [
        "## Dependencies\n",
        "Next, we load the required dependencies. Here we import `ImbalancedLearningRegression` to later apply Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise. In addition, we use `pandas` for data handling, and `seaborn` to visualize our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPB6tSLinAFS"
      },
      "outputs": [],
      "source": [
        "## load dependencies\n",
        "## load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import math\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "#import smogn_boost\n",
        "from ImbalancedLearningRegression import *\n",
        "#import ImbalancedLearningRegression as iblr"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "do8cJ25c_HcF"
      },
      "source": [
        "# Data Description\n",
        "The dataset is sourced from Kaggle with a usability score of 10.0 (Source: https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice). This data contains information about  and goal here is to predict whether an individual would be interested in vehicle insurance, denoted by response column \"response\". The target variable is highly imbalanced - 0: 319594, 1: 62531\n",
        "\n",
        "## Loading the Data\n",
        "Below, we load our data (Imbalanced Insurance Data set), In this case, we name our training set `data` and test data as 'test-data'. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaFdQ2od-qVO"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/aug_train.csv\")\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/aug_test.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D71nm6Co_KK-"
      },
      "source": [
        "## Introduction to SMOGNBoost\n",
        "Here we cover the focus of this example. We call the `smogn_boost` function from this package (`ImbalancedLearningRegression.smogn_boost`) and satisfy the minimum required arguments: `data` and `y`.\n",
        "\n",
        "* data: this argument takes a training data set\n",
        "* test_data: this argument takes a test data set\n",
        "* y: this argument takes a string, which specifies a response variable by header name \n",
        "* TotalIterations: this argument takes a positive integer, which specifies the total number of iterations\n",
        "* pert: perturbation / noise percentage\n",
        "* replace: sampling replacement (bool)\n",
        "* k: num of neighs for over-sampling (pos int)\n",
        "* error_threshold: this argument takes a positive integer, which specifies an error threshold \n",
        "* rel_thres: user defined relevance threshold \n",
        "* samp_method: \"balance or extreme\" - sampling method is perc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qRV9hjPjJVF",
        "outputId": "9c4b115e-1e92-4f75-ecc5-f906fdf0cd6a"
      },
      "outputs": [],
      "source": [
        "## conduct smogn_boost\n",
        "smogn_boost = iblr.smogn_boost(data, test_data, y=\"response\", TotalIterations=\"3\", pert = 0.02, replace = False, k = 5, error_threshold=\"0.2\", rel_thres = 0.5, samp_method = \"balance\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We read the test & training data and split them into features (X) and target value (Y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the test data and split features (X) and target value (Y)\n",
        "df_testData = pd.read_csv(test_data, header = 0)\n",
        "X_test = df_testData.drop(y, axis = 1)\n",
        "Y_test = df_testData[y]\n",
        "    \n",
        "# read the training data and split features (X) and target value (Y)\n",
        "df_data = pd.read_csv(data, header = 0)\n",
        "X_data = df_data.drop(y, axis = 1)\n",
        "Y_data = df_data[y]\n",
        "\n",
        "# set for clarity, name of target variable not data\n",
        "y_train = y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we set an initial iteration as well as initialize empty arrays for the result, beta values, and decision tree predictions based on x_test. The array will store values over each iteration and be used to calculate the result after the final iteration (total iterations specified by the user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set an initial iteration\n",
        "iteration = 1\n",
        "    \n",
        "# set an array of results, beta values, and decision tree predictions based on x_test\n",
        "result = np.empty(TotalIterations, dtype=int)\n",
        "beta = np.empty(TotalIterations, dtype=int)\n",
        "dt_test_predictions = np.empty(X_test, dtype=int)\n",
        "    \n",
        "# Dt(i) set distribution as 1/m weights, which is length of training data -1, as one of them is the target variable y \n",
        "weights = 1/(len(data))\n",
        "dt_distribution = np.zeros(len(data))\n",
        "for i in range(len(data)):\n",
        "    dt_distribution[i] = weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call phi control and specificially the control points as it will be used when we apply SMOGN to oversample the imbalanced training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calling phi control\n",
        "pc = phi_ctrl_pts (y=y, method=\"auto\", xtrm_type = \"both\", coeff = 1.5, ctrl_pts=None)\n",
        "    \n",
        "# calling only the control points (third value) from the output\n",
        "rel_ctrl_pts_rg = pc[2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we begin the main function which runs for the total iterations specified by the user.\n",
        "\n",
        "We obtain an oversampled dataset using SMOGN and our training dataset, split the oversampled data into features and a target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop while iteration is less than user provided iterations\n",
        "while iteration <= TotalIterations:\n",
        "\n",
        "# use initial training data set provided by user to obtain oversampled dataset using SMOGN, calculating it for the bumps\n",
        "    dt_over_sampled = smogn(data=data, y_train = y_train, k = 5, pert = pert, replace=replace, rel_thres = rel_thres, rel_method = \"manual\", rel_ctrl_pts_rg = rel_ctrl_pts_rg)\n",
        "\n",
        "# splitting oversampled data for subsequent training data use below\n",
        "    df_oversampled = dt_over_sampled, header = 0\n",
        "    x_oversampled = df_oversampled.drop(y_train, axis = 1)\n",
        "    y_oversampled = df_oversampled[y_train]\n",
        "    \n",
        "# calls the decision tree and use it to achieve a new model, predict regression value for y (target response variable), and return the predicted values\n",
        "    dt_model = tree.DecisionTreeRegressor()\n",
        "        \n",
        "# train decision tree classifier\n",
        "    dt_model = dt_model.fit(x_oversampled, y_oversampled)\n",
        "        \n",
        "# predict the features in user provided data\n",
        "    dt_data_predictions = dt_model.predict(X_data)\n",
        "        \n",
        "# predict the features in user provided test data\n",
        "    dt_test_predictions.append(dt_model.predict(X_test))\n",
        "\n",
        "# initialize model error rate & epsilon t value\n",
        "    model_error = np.zeros(len(dt_data_predictions))\n",
        "    epsilon_t = 0\n",
        "\n",
        "# calculate the model error rate of the new model achieved earlier, as the delta between original dataset and predicted oversampled dataset\n",
        "# for each y in the dataset, calculate whether it is greater/lower than threshold and update accordingly\n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        model_error[i] = abs((Y_data[i] - dt_data_predictions[i])/Y_data[i])\n",
        "        \n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        if model_error[i] > error_threshold:\n",
        "            epsilon_t = epsilon_t + dt_distribution[i]\n",
        "                                      \n",
        "# beta is the update parameter of weights based on the model error rate calculated\n",
        "    beta.append(pow(epsilon_t, 2))\n",
        "\n",
        "# update the distribution weights\n",
        "    for i in dt_distribution:\n",
        "        if model_error[i] <= error_threshold:\n",
        "            dt_distribution[i] = dt_distribution[i] * beta\n",
        "        else:\n",
        "            dt_distribution[i] = dt_distribution[i]\n",
        "\n",
        "# normalize the distribution \n",
        "    dt_normalized = preprocessing.normalize(dt_distribution, max)\n",
        "\n",
        "# iteration count\n",
        "    iteration += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we calculate the result outside the while loop. We split calculations into numerator and denominator calculating a series. To simplify the calculation, we use log(b) instead of (log(1/b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate result\n",
        "numer = 0\n",
        "denom = 0\n",
        "    \n",
        "for b, i in zip(beta, dt_test_predictions):\n",
        "    numer += math.log(b) * i\n",
        "    denom += math.log(b)\n",
        "return numer/denom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYCW_1t-zra"
      },
      "source": [
        "## Results\n",
        "After conducting SMOGNBoost, we briefly examine the results. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spqtcHX1yTfM",
        "outputId": "bfae806f-6e9c-4db7-a09d-0a7786e5d202"
      },
      "outputs": [],
      "source": [
        "## dimensions - original data \n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5_E-5oQF18"
      },
      "source": [
        "## Conclusion\n",
        "TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQ5iGDJa2LM"
      },
      "source": [
        "## References\n",
        "\n",
        "Branco, P., Torgo, L., Ribeiro, R. (2017). SMOGN: A Pre-Processing Approach for Imbalanced Regression. Proceedings of Machine Learning Research, 74:36-50. http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
        "\n",
        "Torgo, L., Ribeiro, R. P., Pfahringer, B., & Branco, P. (2013, September). Smote for regression. In Portuguese conference on artificial intelligence (pp. 378-389). Springer, Berlin, Heidelberg. https://researchcommons.waikato.ac.nz/bitstream/handle/10289/8518/smoteR.pdf?sequence=23\n",
        "\n",
        "Kunz, N. (2019). SMOGN: Synthetic Minority Over-Sampling for Regression with Gaussian Noise (Version 0.1.0). Python Package Index.\n",
        "https://pypi.org/project/smogn. \n",
        "\n",
        "Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.\n",
        "http://www-bcf.usc.edu/~gareth/ISL/data.html.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMOTE_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('ImbalancedLearningRegression')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "1c961f0653c6e1bdf33e63c7e72c10f24f14dece539fb6df7c7f98656204e204"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
