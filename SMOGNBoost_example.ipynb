{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IoG91SaLwtuq"
      },
      "source": [
        "# ImbalancedLearningRegression (0.0.1): Usage\n",
        "---\n",
        "## SMOGNBoost\n",
        "Amit Shanbhoug, 8677407 \\\n",
        "Adapted from Nick Kunz's SMOGN package: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_1_beg.ipynb\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSHCUHR--nw"
      },
      "source": [
        "## Installation\n",
        "\n",
        "First, we install ImbalancedLearningRegression from the Github repository. Alternatively, we could install from the official PyPI distribution. However, the developer version is utilized here for the latest release. Works on Kernel: python 3.10.7, there may be some issues if you run with a different version, older or newer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9SqBgJ8rduy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## suppress install output\n",
        "\n",
        "## install pypi release\n",
        "# !pip install ImbalancedLearningRegression\n",
        "\n",
        "## install developer version\n",
        "# !pip install git+https://github.com/paobranco/ImbalancedLearningRegression.git"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gKhs9eJd_Ab6"
      },
      "source": [
        "## Dependencies\n",
        "Next, we load the required dependencies. Here we import `ImbalancedLearningRegression` to later apply Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise. In addition, we use `pandas` for data handling, and `seaborn` to visualize our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPB6tSLinAFS"
      },
      "outputs": [],
      "source": [
        "## load dependencies\n",
        "## load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import math\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from ImbalancedLearningRegression.smogn import smogn\n",
        "from ImbalancedLearningRegression.smogn_boost import smogn_boost\n",
        "#from ImbalancedLearningRegression import *\n",
        "import ImbalancedLearningRegression as iblr\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "do8cJ25c_HcF"
      },
      "source": [
        "# Data Description\n",
        "The dataset is sourced from Kaggle with a usability score of 10.0 (Source: https://www.kaggle.com/datasets/arashnic/imbalanced-data-practice). This data contains information about  and goal here is to predict whether an individual would be interested in vehicle insurance, denoted by response column \"response\". The target variable is highly imbalanced - 0: 319594, 1: 62531\n",
        "\n",
        "## Loading the Data\n",
        "Below, we load our data (Imbalanced Insurance Data set), In this case, we name our training set `data` and test data as 'test-data'. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaFdQ2od-qVO"
      },
      "outputs": [],
      "source": [
        "#ds = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/housing.csv\")\n",
        "#ata = \"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/College.csv\"\n",
        "#test_data = \"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/CollegeTest.csv\"\n",
        "#print(len(ds))\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/red_wine.csv\")\n",
        "test_data = pd.read_csv(\"https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/SMOGNBoost/data/red_wine_test.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D71nm6Co_KK-"
      },
      "source": [
        "## Introduction to SMOGNBoost\n",
        "Here we cover the focus of this example. We call the `smogn_boost` function from this package (`ImbalancedLearningRegression.smogn_boost`) and satisfy the minimum required arguments: `data` and `y`.\n",
        "\n",
        "* data: this argument takes a training data set\n",
        "* test_data: this argument takes a test data set\n",
        "* y: this argument takes a string, which specifies a response variable by header name \n",
        "* TotalIterations: this argument takes a positive integer, which specifies the total number of iterations\n",
        "* pert: perturbation / noise percentage\n",
        "* replace: sampling replacement (bool)\n",
        "* k: num of neighs for over-sampling (pos int)\n",
        "* error_threshold: this argument takes a positive integer, which specifies an error threshold \n",
        "* rel_thres: user defined relevance threshold \n",
        "* samp_method: \"balance or extreme\" - sampling method is perc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## conduct smogn\n",
        "#smogn = smogn(data, y= \"SalePrice\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qRV9hjPjJVF",
        "outputId": "9c4b115e-1e92-4f75-ecc5-f906fdf0cd6a"
      },
<<<<<<< HEAD
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:201: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dist_matrix: 100%|##########| 214/214 [00:09<00:00, 23.21it/s]\n",
            "synth_matrix: 100%|##########| 214/214 [00:00<00:00, 1011.78it/s]\n",
            "r_index: 100%|##########| 88/88 [00:00<00:00, 1011.49it/s]\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:289: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:201: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "BETA: 0\n",
            "[0.00064599 0.00064599 0.00064599 ... 0.00064599 0.00064599 0.00064599]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dist_matrix: 100%|##########| 214/214 [00:09<00:00, 23.04it/s]\n",
            "synth_matrix: 100%|##########| 214/214 [00:00<00:00, 1038.76it/s]\n",
            "r_index: 100%|##########| 88/88 [00:00<00:00, 1035.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "BETA: 0\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0.0e+000 5.4e-323 0.0e+000 0.0e+000]\n",
            "[7.71449150e-043 1.91387507e-312 6.00000000e+000 1.00000000e+001\n",
            " 5.00000000e+000 5.00000000e+000 7.00000000e+000 5.00000000e+000\n",
            " 4.00000000e+000 6.00000000e+000 6.00000000e+000 4.00000000e+000\n",
            " 5.00000000e+000 5.00000000e+000 5.00000000e+000 5.00000000e+000\n",
            " 7.00000000e+000 7.00000000e+000 7.00000000e+000 5.00000000e+000\n",
            " 7.00000000e+000 7.00000000e+000 5.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 6.00000000e+000 5.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 7.00000000e+000 1.00000000e+001 5.00000000e+000\n",
            " 6.00000000e+000 6.00000000e+000 6.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 6.00000000e+000 7.00000000e+000 8.00000000e+000\n",
            " 5.00000000e+000 6.00000000e+000 7.00000000e+000 5.00000000e+000\n",
            " 7.00000000e+000 6.00000000e+000 6.00000000e+000 5.00000000e+000\n",
            " 6.00000000e+000 5.00000000e+000 6.00000000e+000 6.00000000e+000\n",
            " 8.15384615e+000 5.00000000e+000 1.00000000e+001 5.00000000e+000\n",
            " 5.00000000e+000 6.00000000e+000 5.00000000e+000 7.00000000e+000\n",
            " 6.00000000e+000 6.00000000e+000 7.00000000e+000 5.00000000e+000\n",
            " 5.00000000e+000 5.00000000e+000 5.00000000e+000 5.00000000e+000\n",
            " 5.00000000e+000 5.00000000e+000 6.00000000e+000 7.00000000e+000\n",
            " 5.00000000e+000 5.00000000e+000 1.00000000e+001 7.00000000e+000\n",
            " 1.20000000e+001 5.00000000e+000 7.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 6.00000000e+000 1.00000000e+001 6.00000000e+000\n",
            " 6.00000000e+000 7.00000000e+000 6.00000000e+000 5.00000000e+000\n",
            " 6.00000000e+000 8.28571429e+000 6.00000000e+000 5.00000000e+000\n",
            " 6.00000000e+000 7.00000000e+000 6.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 7.60000000e+000 4.00000000e+000 6.00000000e+000\n",
            " 6.00000000e+000 7.60000000e+000 5.00000000e+000 6.00000000e+000]\n",
            "nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn.py:289: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:189: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  numer += math.log(1/b) * i\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:190: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  denom += math.log(1/b)\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:189: RuntimeWarning: overflow encountered in double_scalars\n",
            "  numer += math.log(1/b) * i\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:190: RuntimeWarning: overflow encountered in double_scalars\n",
            "  denom += math.log(1/b)\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:191: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  print(numer/denom)\n",
            "c:\\Users\\Cameron Cooke\\OneDrive\\Documents\\GitHub\\ImbalancedLearningRegression\\ImbalancedLearningRegression\\smogn_boost.py:192: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return numer/denom\n"
          ]
        }
      ],
=======
      "outputs": [],
>>>>>>> 340d3c1b220a7dc84b9d065a14eeab28c685a624
      "source": [
        "## conduct smogn_boost\n",
        "smogn_boost = smogn_boost(data, test_data, y=\"volatile acidity\", TotalIterations=2)"
      ]
    },
    {
<<<<<<< HEAD
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We read the test & training data and split them into features (X) and target value (Y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "argument of type 'method' is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# read the test data and split features (X) and target value (Y)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_testData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(test_data, header \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m X_test \u001b[39m=\u001b[39m df_testData\u001b[39m.\u001b[39mdrop(y, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m Y_test \u001b[39m=\u001b[39m df_testData[y]\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:704\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    701\u001b[0m errors \u001b[39m=\u001b[39m errors \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[39m# read_csv does not know whether the buffer is opened in binary/text mode\u001b[39;00m\n\u001b[1;32m--> 704\u001b[0m \u001b[39mif\u001b[39;00m _is_binary_mode(path_or_buf, mode) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    705\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m \u001b[39m# validate encoding and errors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Cameron Cooke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:1163\u001b[0m, in \u001b[0;36m_is_binary_mode\u001b[1;34m(handle, mode)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(handle), text_classes):\n\u001b[0;32m   1161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, _get_binary_io_classes()) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m \u001b[39mgetattr\u001b[39;49m(\n\u001b[0;32m   1164\u001b[0m     handle, \u001b[39m\"\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\n\u001b[0;32m   1165\u001b[0m )\n",
            "\u001b[1;31mTypeError\u001b[0m: argument of type 'method' is not iterable"
          ]
        }
      ],
      "source": [
        "# read the test data and split features (X) and target value (Y)\n",
        "df_testData = pd.read_csv(test_data, header = 0)\n",
        "X_test = df_testData.drop(y, axis = 1)\n",
        "Y_test = df_testData[y]\n",
        "    \n",
        "# read the training data and split features (X) and target value (Y)\n",
        "df_data = pd.read_csv(data, header = 0)\n",
        "X_data = df_data.drop(y, axis = 1)\n",
        "Y_data = df_data[y]\n",
        "\n",
        "# set for clarity, name of target variable not data\n",
        "y_train = y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we set an initial iteration as well as initialize empty arrays for the result, beta values, and decision tree predictions based on x_test. The array will store values over each iteration and be used to calculate the result after the final iteration (total iterations specified by the user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set an initial iteration\n",
        "iteration = 1\n",
        "    \n",
        "# set an array of results, beta values, and decision tree predictions based on x_test\n",
        "result = np.empty(TotalIterations, dtype=int)\n",
        "beta = np.empty(TotalIterations, dtype=int)\n",
        "dt_test_predictions = np.empty(X_test, dtype=int)\n",
        "    \n",
        "# Dt(i) set distribution as 1/m weights, which is length of training data -1, as one of them is the target variable y \n",
        "weights = 1/(len(data))\n",
        "dt_distribution = np.zeros(len(data))\n",
        "for i in range(len(data)):\n",
        "    dt_distribution[i] = weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We call phi control and specificially the control points as it will be used when we apply SMOGN to oversample the imbalanced training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calling phi control\n",
        "pc = phi_ctrl_pts (y=y, method=\"auto\", xtrm_type = \"both\", coeff = 1.5, ctrl_pts=None)\n",
        "    \n",
        "# calling only the control points (third value) from the output\n",
        "rel_ctrl_pts_rg = pc[2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we begin the main function which runs for the total iterations specified by the user.\n",
        "\n",
        "We obtain an oversampled dataset using SMOGN and our training dataset, split the oversampled data into features and a target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop while iteration is less than user provided iterations\n",
        "while iteration <= TotalIterations:\n",
        "\n",
        "# use initial training data set provided by user to obtain oversampled dataset using SMOGN, calculating it for the bumps\n",
        "    dt_over_sampled = smogn(data=data, y_train = y_train, k = 5, pert = pert, replace=replace, rel_thres = rel_thres, rel_method = \"manual\", rel_ctrl_pts_rg = rel_ctrl_pts_rg)\n",
        "\n",
        "# splitting oversampled data for subsequent training data use below\n",
        "    df_oversampled = dt_over_sampled, header = 0\n",
        "    x_oversampled = df_oversampled.drop(y_train, axis = 1)\n",
        "    y_oversampled = df_oversampled[y_train]\n",
        "    \n",
        "# calls the decision tree and use it to achieve a new model, predict regression value for y (target response variable), and return the predicted values\n",
        "    dt_model = tree.DecisionTreeRegressor()\n",
        "        \n",
        "# train decision tree classifier\n",
        "    dt_model = dt_model.fit(x_oversampled, y_oversampled)\n",
        "        \n",
        "# predict the features in user provided data\n",
        "    dt_data_predictions = dt_model.predict(X_data)\n",
        "        \n",
        "# predict the features in user provided test data\n",
        "    dt_test_predictions.append(dt_model.predict(X_test))\n",
        "\n",
        "# initialize model error rate & epsilon t value\n",
        "    model_error = np.zeros(len(dt_data_predictions))\n",
        "    epsilon_t = 0\n",
        "\n",
        "# calculate the model error rate of the new model achieved earlier, as the delta between original dataset and predicted oversampled dataset\n",
        "# for each y in the dataset, calculate whether it is greater/lower than threshold and update accordingly\n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        model_error[i] = abs((Y_data[i] - dt_data_predictions[i])/Y_data[i])\n",
        "        \n",
        "    for i in range(len(dt_data_predictions)):\n",
        "        if model_error[i] > error_threshold:\n",
        "            epsilon_t = epsilon_t + dt_distribution[i]\n",
        "                                      \n",
        "# beta is the update parameter of weights based on the model error rate calculated\n",
        "    beta.append(pow(epsilon_t, 2))\n",
        "\n",
        "# update the distribution weights\n",
        "    for i in dt_distribution:\n",
        "        if model_error[i] <= error_threshold:\n",
        "            dt_distribution[i] = dt_distribution[i] * beta\n",
        "        else:\n",
        "            dt_distribution[i] = dt_distribution[i]\n",
        "\n",
        "# normalize the distribution \n",
        "    dt_normalized = preprocessing.normalize(dt_distribution, max)\n",
        "\n",
        "# iteration count\n",
        "    iteration += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we calculate the result outside the while loop. We split calculations into numerator and denominator calculating a series. To simplify the calculation, we use log(b) instead of (log(1/b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate result\n",
        "numer = 0\n",
        "denom = 0\n",
        "    \n",
        "for b, i in zip(beta, dt_test_predictions):\n",
        "    numer += math.log(b) * i\n",
        "    denom += math.log(b)\n",
        "return numer/denom"
      ]
    },
    {
=======
>>>>>>> 340d3c1b220a7dc84b9d065a14eeab28c685a624
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYCW_1t-zra"
      },
      "source": [
        "## Results\n",
        "After conducting SMOGNBoost, we briefly examine the results. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA5_E-5oQF18"
      },
      "source": [
        "## Conclusion\n",
        "TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQ5iGDJa2LM"
      },
      "source": [
        "## References\n",
        "\n",
        "Branco, P., Torgo, L., Ribeiro, R. (2017). SMOGN: A Pre-Processing Approach for Imbalanced Regression. Proceedings of Machine Learning Research, 74:36-50. http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
        "\n",
        "Torgo, L., Ribeiro, R. P., Pfahringer, B., & Branco, P. (2013, September). Smote for regression. In Portuguese conference on artificial intelligence (pp. 378-389). Springer, Berlin, Heidelberg. https://researchcommons.waikato.ac.nz/bitstream/handle/10289/8518/smoteR.pdf?sequence=23\n",
        "\n",
        "Kunz, N. (2019). SMOGN: Synthetic Minority Over-Sampling for Regression with Gaussian Noise (Version 0.1.0). Python Package Index.\n",
        "https://pypi.org/project/smogn. \n",
        "\n",
        "Gareth, J., Daniela, W., Trevor, H., & Robert, T. (2013). An introduction to statistical learning: with applications in R. Spinger.\n",
        "http://www-bcf.usc.edu/~gareth/ISL/data.html.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMOTE_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('ImbalancedLearningRegression')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "1c961f0653c6e1bdf33e63c7e72c10f24f14dece539fb6df7c7f98656204e204"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
