ImbalancedLearningRegression (0.0.1): Usage
Nearmiss Undersampling
Cameron Cooke, ccook059@uottawa.ca
Adapted from Nick Kunz's SMOGN package: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_1_beg.ipynb

Installation
First, we install ImbalancedLearningRegression from the Github repository. Alternatively, we could install from the official PyPI distribution. However, the developer version is utilized here for the latest release.

%%capture
## suppress install output

## install pypi release
# !pip install ImbalancedLearningRegression

## install developer version
!pip install git+https://github.com/paobranco/ImbalancedLearningRegression.git
Dependencies
Next, we load the required dependencies. Here we import ImbalancedLearningRegression to later apply Nearmiss algorithm for under-sampling regression data. In addition, we use pandas for data handling, and seaborn to visualize our results.

## load dependencies
import ImbalancedLearningRegression as iblr
import pandas
import seaborn
import matplotlib.pyplot as plt
Data
After, we load our data. In this example, we use the Ames Housing Dataset training split retrieved from Kaggle, originally complied by Dean De Cock. In this case, we name our training set housing

## load data
housing = pandas.read_csv(

    ## http://jse.amstat.org/v19n3/decock.pdf
    'https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/master/data/housing.csv'
)
Nearmiss
Here we cover the focus of this example. We call the nearmiss function from this package (ImbalancedLearningRegression.enn) and satisfy the minimum required arguments: data and y.

The data argument takes a Pandas DataFrame, which contains the training set split. In this example, we input the previously loaded housing training set with follow input: data = housing

The y argument takes a string, which specifies a continuous reponse variable by header name. In this example, we input 'SalePrice' in the interest of predicting the sale price of homes in Ames, Iowa with the following input: y = 'SalePrice'

The version argument takes an int (1, 2 or 3) and specifies which version of nearmiss will be used. See documentation for further clarification.
## conduct nearmiss
housing_nearmiss = iblr.nearmiss(
    
    data = housing,  ## pandas dataframe
    y = 'SalePrice',  ## string ('header name'),
    version = 1
)
Note:

In this example, the regions of interest within the response variable y are automatically determined by the box plot extremes. The extreme values are considered rare "minorty" values are over-sampled. The values closer the median are considered "majority" values and are under-sampled.

If there are no box plot extremes contained in the reponse variable y, the argument rel_method = manual must be specified, and an input matrix must be placed into the argument rel_ctrl_pts_rg indicating the regions of rarity in y.

More information regarding the matrix input to the rel_ctrl_pts_rg argument and manual over-sampling can be found within the function's doc string, as well as in Nick Kunz's package SMOGN: https://github.com/nickkunz/smogn/blob/master/examples/smogn_example_3_adv.ipynb.

It is also important to mention that by default, ImbalancedLearningRegression.nearmiss will first automatically remove columns containing missing values and then remove rows, as it cannot input data containing missing values.

Results
After conducting Nearmiss algorithm, we briefly examine the results.

We can see that the number of observations (rows) in the original training set decreased from __________, while the number of features (columns) also decreased from _______.

Recall that ImbalancedLearningRegression.nearmiss automatically removes features containing missing values. In this case, _________ features contained missing values and were therefore omitted.

The reduction in observations were a result of under-sampling. More detailed information in this regard can be found in the original paper cited in the References section.

## dimensions - original data 
housing.shape
(1460, 81)
## dimensions - modified data
housing_nearmiss.shape
(1428, 62)
Further examining the results, we can see that the distribution of the response variable has changed. By calling the box_plot_stats function from this package (ImbalancedLearningRegression.box_plot_stats) we quickly verify.

Notice that the modified training set's box plot five number summary has changed, where the distribution of the response variable has slightly skewed left when compared to the original training set.

## box plot stats - original data 
iblr.box_plot_stats(housing['SalePrice'])['stats']
array([ 34900., 129950., 163000., 214000., 340000.])
## box plot stats - modified data
iblr.box_plot_stats(housing_nearmiss['SalePrice'])['stats']
array([ 34900., 129250., 161000., 210500., 328900.])
Plotting the results of both the original and modified training sets, the skewed left distribution of the response variable in the modified training set is still not evident, because the original dataset only shrank a tiny little bit.

In this example, Nearmiss under-sampled observations that were closer to the median (those considered "majority").

This is the quickest implementation when the y values of interest in predicting may be unclear within a given dataset.

## plot y distribution 
seaborn.kdeplot(housing['SalePrice'], label = "Original")
seaborn.kdeplot(housing_nearmiss['SalePrice'], label = "Modified")
plt.legend(labels=["Original","Modified"])
<matplotlib.legend.Legend at 0x1ee615125f0>

More datasets
To verify the reliability of the implementation, we test it with more datasets; this time using a different version of Nearmiss.

We use the College dataset retrieved from Kaggle, uploaded by Fares Sayah. In this case, we name our training set college. We run the same procedures as previously shown.

college = pandas.read_csv(

    'https://raw.githubusercontent.com/paobranco/ImbalancedLearningRegression/master/data/College.csv'
)

## conduct enn
college_nearmiss = iblr.nearmiss(
    
    data = college,  ## pandas dataframe
    y = 'Grad.Rate', ## string ('header name')
    version = 2 

## dimensions - original data 
college.shape
(777, 19)
## dimensions - modified data
college_nearmiss.shape
(763, 19)
## box plot stats - original data 
iblr.box_plot_stats(college['Grad.Rate'])['stats']
array([ 18.,  53.,  65.,  78., 100.])
## box plot stats - modified data
iblr.box_plot_stats(college_nearmiss['Grad.Rate'])['stats']
array([ 18.,  53.,  65.,  77., 100.])
## plot y distribution 
seaborn.kdeplot(college['Grad.Rate'], label = "Original")
seaborn.kdeplot(college_nearmiss['Grad.Rate'], label = "Modified")
plt.legend(labels=["Original","Modified"])
<matplotlib.legend.Legend at 0x1ee61112110>

Results 2
We can see that the number of observations (rows) in the original training set decreased from 777 to 763, while the number of features (columns) didn't change.

The decrease in observations were a result of under-sampling.

Further examining the results, we can see that the distribution of the response variable has changed. By calling the box_plot_stats function from this package (ImbalancedLearningRegression.box_plot_stats) we quickly verify.

Notice that the modified training set's box plot five number summary has changed, where the distribution of the response variable has slightly skewed left for only the fourth point when compared to the original training set.

Plotting the results of both the original and modified training sets, the skewed left distribution of the response variable in the modified training set is still not evident, because the original dataset only shrank a tiny little bit.

In this example, ENN under-sampled observations whose 'Grad.Rate' was found to be closer to the median (those considered "majority").

This is the quickest implementation when the y values of interest in predicting may be unclear within a given dataset.

Note that some values generated are invalid for the 'Grad.Rate' attribute. Users are required to detect and manipulate the potential likelihood of generation of noise values.

Conclusion
In the examples, we covered the fundamentals of ENN in ImbalancedLearningRegression. We utilized the Ames Housing Dataset and College dataset to conduct Edited Nearest Neighbour algorithm with the minimum required arguments.

ENN under-sampled "majority" values in a continuous response variable. The results were briefly examined with the box plot's five number summary and plotted for visual confirmation.

References
